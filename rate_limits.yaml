# Rate limit configuration for API providers and models
openai:
  gpt-4o:
    requests_per_minute: 5_000
    tokens_per_minute: 800_000
  gpt-4o-mini:
    requests_per_minute: 5_000
    tokens_per_minute: 100_000_000
  o4-mini:
    requests_per_minute: 500
    tokens_per_minute: 200_000
  o3:
    requests_per_minute: 10_000
    tokens_per_minute: 2_000_000
  gpt-4.1:
    requests_per_minute: 10_000
    tokens_per_minute: 2_000_000
  gpt-4.1-mini:
    requests_per_minute: 10_000
    tokens_per_minute: 10_000_000
  gpt-4.1-nano:
    requests_per_minute: 10_000
    tokens_per_minute: 10_000_000

anthropic:
  claude-3-7-sonnet-20250219:
    requests_per_minute: 1_000
    tokens_per_minute: 40_000

  claude-3-5-sonnet-20241022:
    requests_per_minute: 1_000
    tokens_per_minute: 80_000

  claude-3-5-haiku-20241022:
    requests_per_minute: 1_000
    tokens_per_minute: 100_000

# gemini:
#   gemini/gemini-2.0-flash:
#     requests_per_minute: 2_000
#     tokens_per_minute: 4_000_000

#   gemini/gemini-2.0-pro-exp:
#     requests_per_minute: 20
#     tokens_per_minute: 2_000_000

#   gemini/gemini-2.5-pro-preview-03-25:
#     requests_per_minute: 150
#     tokens_per_minute: 2_000_000

#   gemini/gemini-2.5-flash-preview-04-17:
#     requests_per_minute: 150
#     tokens_per_minute: 2_000_000

#   gemini/gemini-2.5-pro-preview-05-06:
#     requests_per_minute: 150
#     tokens_per_minute: 2_000_000
